{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMoMK7tH3IfuE0aS+sW6YWI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jawakar-7/Image-Classification-using-multiple-models-/blob/main/Transfer_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fD3LNrN6qPSd",
        "outputId": "c0c10d34-559e-4fe1-c216-4bed1332cef4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Image-Classification-using-multiple-models-'...\n",
            "remote: Enumerating objects: 24370, done.\u001b[K\n",
            "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
            "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "remote: Total 24370 (delta 19), reused 21 (delta 7), pack-reused 24316\u001b[K\n",
            "Receiving objects: 100% (24370/24370), 369.25 MiB | 16.82 MiB/s, done.\n",
            "Resolving deltas: 100% (20/20), done.\n",
            "Updating files: 100% (24344/24344), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Jawakar-7/Image-Classification-using-multiple-models-.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "MD19y0lvqUP3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_path='/content/Image-Classification-using-multiple-models-/Datasets/seg_test/seg_test'\n",
        "train_path='/content/Image-Classification-using-multiple-models-/Datasets/seg_train/seg_train'\n",
        ""
      ],
      "metadata": {
        "id": "LExyK47hqXDT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import column_or_1d\n",
        "train_xgen=ImageDataGenerator(\n",
        "    rescale=1.0/255,#Scaling Down the data\n",
        "    zoom_range=0.1,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.1,\n",
        ")\n",
        "X_train=train_xgen.flow_from_directory(\n",
        "    train_path,\n",
        "    target_size=(150,150),\n",
        "    color_mode='rgb',\n",
        "    class_mode='categorical',\n",
        "    batch_size=32\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0P-X5ruqZiq",
        "outputId": "a52efaed-b1b3-4039-8c7a-90c6fe13194d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 14034 images belonging to 6 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_xgen=ImageDataGenerator(\n",
        "    rescale=1.0/255\n",
        ")\n",
        "X_test=test_xgen.flow_from_directory(\n",
        "    test_path,\n",
        "    class_mode='categorical',\n",
        "    target_size=(150,150),\n",
        "    batch_size=32,\n",
        "    color_mode='rgb',\n",
        "    shuffle=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sP3qr91gqb20",
        "outputId": "b47fff17-5615-4c00-c4ce-dc7641299f4b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3000 images belonging to 6 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "labels={i:k for k,i in X_train.class_indices.items()}\n",
        "for i,k in labels.items():\n",
        "  print(f\"{i}:{k}\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OP4-PlAXqjHH",
        "outputId": "e9a8a7f3-0b67-40d6-ec17-63e636f4594f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:buildings\n",
            "1:forest\n",
            "2:glacier\n",
            "3:mountain\n",
            "4:sea\n",
            "5:street\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{len(X_train)} which is total images/32\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71qC0OBgqm5t",
        "outputId": "134299f6-7843-4c82-b8d9-436e96ea1d65"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "439 which is total images/32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i,k in X_train:\n",
        "  print(f\"{i.shape}is a tuple that contains(batch size,target size and channel_size,) and k is the corresponding classe of  image {labels[np.argmax(k[0],axis=0)]} meaning the last class\")\n",
        "  break\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNA2HX0vqoig",
        "outputId": "1f8326ad-5512-4c8b-cf87-7e7dc3803d85"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 150, 150, 3)is a tuple that contains(batch size,target size and channel_size,) and k is the corresponding classe of  image mountain meaning the last class\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0][1][0]#accesing the label of first element from the first batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niRhubY4qqKO",
        "outputId": "ad584386-6988-40f0-e861-10a101192cc1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 1., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inception V3"
      ],
      "metadata": {
        "id": "p8EhAUZ23DES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "# local_weights_file = '/kaggle/input/inceptionv3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "pre_trained_model = InceptionV3(input_shape = (150, 150, 3),\n",
        "                                include_top = False,\n",
        "                                weights = \"imagenet\")\n",
        "\n",
        "# pre_trained_model.load_weights(local_weights_file)\n",
        "\n",
        "for layer in pre_trained_model.layers:\n",
        "     layer.trainable = False\n",
        "\n",
        "# pre_trained_model.summary()\n",
        "last_layer = pre_trained_model.get_layer('mixed7')\n",
        "print('last layer output shape: ', last_layer.output_shape)\n",
        "last_output = last_layer.output\n",
        "\n",
        "x = layers.Flatten()(last_output)\n",
        "x = layers.Dense(1024, activation='relu')(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "x = layers.Dense(6, activation='softmax')(x)\n",
        "\n",
        "model = Model(pre_trained_model.input, x)\n",
        "\n",
        "model.compile(optimizer = RMSprop(learning_rate=0.0001),\n",
        "              loss = CategoricalCrossentropy(),\n",
        "              metrics = ['acc'])\n",
        "\n",
        "history=model.fit(X_train,epochs=5,validation_data=X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ot_h--OHqseP",
        "outputId": "ce9f1732-c040-441c-d406-245e6e0b8294"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "last layer output shape:  (None, 7, 7, 768)\n",
            "Epoch 1/5\n",
            "439/439 [==============================] - 97s 214ms/step - loss: 0.4370 - acc: 0.8468 - val_loss: 0.2609 - val_acc: 0.9017\n",
            "Epoch 2/5\n",
            "439/439 [==============================] - 91s 208ms/step - loss: 0.3075 - acc: 0.8897 - val_loss: 0.2400 - val_acc: 0.9170\n",
            "Epoch 3/5\n",
            "439/439 [==============================] - 93s 212ms/step - loss: 0.2657 - acc: 0.9033 - val_loss: 0.2323 - val_acc: 0.9123\n",
            "Epoch 4/5\n",
            "439/439 [==============================] - 91s 208ms/step - loss: 0.2403 - acc: 0.9141 - val_loss: 0.2368 - val_acc: 0.9170\n",
            "Epoch 5/5\n",
            "439/439 [==============================] - 92s 209ms/step - loss: 0.2270 - acc: 0.9200 - val_loss: 0.2432 - val_acc: 0.9253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VGG 16"
      ],
      "metadata": {
        "id": "9PYzHTj33GrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "pretrained_model=VGG16(input_shape = (150, 150, 3),\n",
        "                        include_top = False,\n",
        "                        weights = 'imagenet')\n",
        "\n",
        "for layer in pretrained_model.layers:\n",
        "     layer.trainable = False\n",
        "\n",
        "# pretrained_model.summary()\n",
        "last_layer = pretrained_model.get_layer('block5_pool')\n",
        "print('last layer of vgg : output shape: ', last_layer.output_shape)\n",
        "last_output= last_layer.output\n",
        "\n",
        "x = layers.Flatten()(last_output)\n",
        "x = layers.Dense(1024, activation='relu')(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "x = layers.Dense(6, activation='softmax')(x)\n",
        "\n",
        "model_vgg = Model(pretrained_model.input, x)\n",
        "\n",
        "\n",
        "model_vgg.compile(optimizer = RMSprop(learning_rate=0.0001),\n",
        "              loss = CategoricalCrossentropy(),\n",
        "              metrics = ['acc'])\n",
        "\n",
        "model_vgg.fit(X_train,epochs=5,validation_data=X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7BZLxURs-kv",
        "outputId": "a9d01a38-1ad0-455a-a49b-afa019325465"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 3s 0us/step\n",
            "last layer of vgg : output shape:  (None, 4, 4, 512)\n",
            "Epoch 1/5\n",
            "439/439 [==============================] - 107s 235ms/step - loss: 0.5808 - acc: 0.7805 - val_loss: 0.3633 - val_acc: 0.8650\n",
            "Epoch 2/5\n",
            "439/439 [==============================] - 101s 230ms/step - loss: 0.4254 - acc: 0.8447 - val_loss: 0.3860 - val_acc: 0.8580\n",
            "Epoch 3/5\n",
            "439/439 [==============================] - 99s 225ms/step - loss: 0.3932 - acc: 0.8554 - val_loss: 0.3782 - val_acc: 0.8580\n",
            "Epoch 4/5\n",
            "439/439 [==============================] - 100s 227ms/step - loss: 0.3627 - acc: 0.8677 - val_loss: 0.3234 - val_acc: 0.8787\n",
            "Epoch 5/5\n",
            "439/439 [==============================] - 103s 234ms/step - loss: 0.3499 - acc: 0.8709 - val_loss: 0.3318 - val_acc: 0.8787\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f1bb06fb640>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resnet 50"
      ],
      "metadata": {
        "id": "yO1emPEU3JKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "#step1\n",
        "# file_resnet='/kaggle/input/vgg16/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "pretrained_model=ResNet50( input_shape=(150,150,3),\n",
        "                                  include_top=False,\n",
        "                                  weights='imagenet'\n",
        "                                   )\n",
        "#step2\n",
        "for layer in pretrained_model.layers:\n",
        "     layer.trainable = False\n",
        "\n",
        "# pretrained_model.summary()\n",
        "\n",
        "#step3\n",
        "last_layer = pretrained_model.get_layer('conv5_block3_out')\n",
        "print('last layer of vgg : output shape: ', last_layer.output_shape)\n",
        "last_output = last_layer.output\n",
        "\n",
        "#step4\n",
        "x = layers.Flatten()(last_output)\n",
        "x = layers.Dense(1024, activation='relu')(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "x = layers.Dense(6, activation='softmax')(x)\n",
        "\n",
        "#step5\n",
        "model_resnet = Model(pretrained_model.input, x)\n",
        "\n",
        "#step6\n",
        "model_resnet.compile(optimizer = RMSprop(learning_rate=0.0001),\n",
        "              loss = CategoricalCrossentropy(),\n",
        "              metrics = ['acc'])\n",
        "model_resnet.fit(X_train,epochs=5,validation_data=X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgtuFncjvHDc",
        "outputId": "f2fd7657-a38e-45e0-de3f-998c5e4677a8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 5s 0us/step\n",
            "last layer of vgg : output shape:  (None, 5, 5, 2048)\n",
            "Epoch 1/5\n",
            "439/439 [==============================] - 105s 231ms/step - loss: 1.5661 - acc: 0.3670 - val_loss: 1.3604 - val_acc: 0.3933\n",
            "Epoch 2/5\n",
            "439/439 [==============================] - 101s 231ms/step - loss: 1.2316 - acc: 0.4857 - val_loss: 1.1279 - val_acc: 0.5270\n",
            "Epoch 3/5\n",
            "439/439 [==============================] - 101s 229ms/step - loss: 1.1449 - acc: 0.5289 - val_loss: 1.0718 - val_acc: 0.5647\n",
            "Epoch 4/5\n",
            "439/439 [==============================] - 101s 230ms/step - loss: 1.0859 - acc: 0.5594 - val_loss: 0.9812 - val_acc: 0.6013\n",
            "Epoch 5/5\n",
            "439/439 [==============================] - 102s 233ms/step - loss: 1.0524 - acc: 0.5764 - val_loss: 1.0425 - val_acc: 0.5663\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f1c3682fd30>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0aQaDVzuw1mQ"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}